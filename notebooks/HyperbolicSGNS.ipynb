{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868c86a0-d794-46f3-b3c3-50f82698d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import geoopt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d40cb0fa-5876-4466-8fc4-bb0e6a45892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManifoldEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, manifold, num_embeddings, embedding_dim, dtype=torch.double, requires_grad=True, weights=None):\n",
    "        super().__init__()\n",
    "        if dtype != torch.double:\n",
    "            logging.warning(\"Double precision is recommended for embeddings on manifold\")\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self._manifold = manifold\n",
    "        if weights is None:\n",
    "            data = torch.zeros((num_embeddings, embedding_dim), dtype=dtype)\n",
    "            data = geoopt.ManifoldTensor(data, manifold=self._manifold)\n",
    "            self.w = geoopt.ManifoldParameter(data, requires_grad=requires_grad)\n",
    "            self.reset_parameters()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        s0 = x.shape\n",
    "        ws = self.w[x.view(-1)]\n",
    "        return ws.view(*s0, self.embedding_dim)\n",
    "    \n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.normal_(self.w.data)\n",
    "        self.w.data[:] = self._manifold.retr(torch.zeros(self.embedding_dim), self.w.data)\n",
    "        \n",
    "        \n",
    "class LorentzEmbedding(ManifoldEmbedding):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, k=1.0, **kwargs):\n",
    "        manifold = geoopt.manifolds.Lorentz(k, learnable=False)\n",
    "        super().__init__(manifold, num_embeddings, embedding_dim, **kwargs)\n",
    "        \n",
    "        \n",
    "class LorentzSkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, k=1.0):\n",
    "        super().__init__()\n",
    "        self._manifold = geoopt.manifolds.Lorentz(k)\n",
    "        self.x0 = torch.zeros(10)\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        return self._manifold.inner(self.x0, a, b)\n",
    "    \n",
    "\n",
    "class SGNSLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, y_, y):\n",
    "        y.masked_fill_(y == 0, -1)\n",
    "        loss = torch.log(torch.sigmoid(y*y_))\n",
    "        if self.reduction is None:\n",
    "            return loss\n",
    "        elif self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.rediction == \"sum\":\n",
    "            return loss.sum()\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "980f3464-66c3-4375-b51b-5ebaafaf367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, theta, k=1.0):\n",
    "        super().__init__()\n",
    "        self.embd = LorentzEmbedding(num_embeddings, embedding_dim, k)\n",
    "        self.sg = LorentzSkipGram(k)\n",
    "        self.loss_fn = SGNSLoss()\n",
    "        self.theta = theta\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        e1, e2 = x[:, 0], x[:, 1]\n",
    "        z = self.sg(e1, e2) + self.theta\n",
    "        return self.loss_fn(z, y)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = geoopt.optim.RiemannianAdam(self.parameters(), 1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e8b42444-d64c-4476-8a77-a311c128bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def skip_gram(x, i, w):\n",
    "    return x[i], x[max(0, i-w):i] + x[i+1:i+w+1]\n",
    "\n",
    "class SkipGramWithNegativeSampling:\n",
    "    \n",
    "    def __init__(self, window, vocabulary, negative=5, negative_probs=None):\n",
    "        self.window = window\n",
    "        self.vocabulary = vocabulary\n",
    "        self.negative = negative\n",
    "        self.negative_probs = negative_probs\n",
    "        \n",
    "    def sample_negatives(self, query):\n",
    "        if self.negative == 0:\n",
    "            return []\n",
    "        assert self.negative_probs is None, \"Weighted sampling not implemented yet!\"\n",
    "        items = set(query)\n",
    "        randoms = random.choices(self.vocabulary, k=len(items)*self.negative)\n",
    "        return zip(list(items)*self.negative, randoms)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        grams = [skip_gram(x, i, self.window) for i in range(len(x))]\n",
    "        batches = [[w,c] for w,context in grams for c in context]\n",
    "        negatives = list(self.sample_negatives(x))\n",
    "        labels = [1] * len(batches) + [0] * len(negatives)\n",
    "        return batches + negatives, labels\n",
    "    \n",
    "    \n",
    "class ToTensor:\n",
    "    \n",
    "    def __init__(self, *dtypes):\n",
    "        self.dtypes = dtypes\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        assert isinstance(x, tuple)\n",
    "        assert len(x) == len(self.dtypes), f\"Number of inputs {len(x)} does not match number of specified data types {len(self.dtypes)}\"\n",
    "        return tuple(torch.tensor(xi, dtype=di) for xi, di in zip(x, self.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "66ffc02e-1bbf-46dc-9b38-6180a2b5ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/\")\n",
    "from models.transformer.loader import PlaylistDataset\n",
    "from models.transformer.transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f21b701c-a6ae-47d4-badf-95911a2e9ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# utils to create this file list\n",
    "\n",
    "def get_file_list(base):\n",
    "    return [os.path.join(base, f) for f in os.listdir(base) if \".json\" in f]\n",
    "\n",
    "files = get_file_list(\"../data/processed/\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "95d6fd1a-3416-45d6-a7b9-68fad51fc3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:26<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Compute and save song frequencies\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "songs = Counter()\n",
    "for f in tqdm(files):\n",
    "    with open(f) as f:\n",
    "        data = json.load(f)[\"playlists\"]\n",
    "        for pl in data:\n",
    "            songs.update(pl)\n",
    "            \n",
    "with open(\"../data/frequencies.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dict(songs)))\n",
    "len(songs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c2418534-bd7a-4408-8a5d-54b20e7a60ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599341"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "MIN_FREQ = 5\n",
    "\n",
    "with open(\"../data/frequencies.json\") as f:\n",
    "    frequencies = json.load(f)\n",
    "\n",
    "frequencies = dict(filter(lambda item: item[1] >= MIN_FREQ, frequencies.items()))\n",
    "songs = list(set(frequencies.keys()))\n",
    "song2idx = {s: i for i,s in enumerate(songs)}\n",
    "idx2song = {i: s for s,i in song2idx.items()}\n",
    "len(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8a497b8d-d87c-4323-9af6-90c43e3eac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    pairs, labels = list(zip(*data))\n",
    "    return torch.cat(pairs, dim=0), torch.cat(labels, dim=0)\n",
    "\n",
    "tf = Compose(\n",
    "    RemoveUnknownTracks(songs),\n",
    "    TrackURI2Idx(song2idx),\n",
    "    SkipGramWithNegativeSampling(5, list(song2idx.values()), 10),\n",
    "    ToTensor(torch.long, torch.float)\n",
    ")\n",
    "dataset = PlaylistDataset(files, 50_000, transform=tf)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=5, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dc76c2e5-92db-4bf9-9dd8-993920a645b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMBEDDING = len(songs)\n",
    "EMBEDDING_DIM = 16\n",
    "THETA = 0\n",
    "\n",
    "model = Model(NUM_EMBEDDING, EMBEDDING_DIM, THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "312c082f-a8d6-42b4-9bd9-60c0bd31f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "da433127-01ea-41ed-becb-7cce9f05829a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | embd    | LorentzEmbedding | 9.6 M \n",
      "1 | sg      | LorentzSkipGram  | 1     \n",
      "2 | loss_fn | SGNSLoss         | 0     \n",
      "---------------------------------------------\n",
      "9.6 M     Trainable params\n",
      "2         Non-trainable params\n",
      "9.6 M     Total params\n",
      "38.358    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                       | 0/200000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:978\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m--> 978\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:201\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:354\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:218\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:185\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         closure()\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:261\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1265\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1232\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m    Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;124;03m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1265\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:158\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:224\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:114\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/geoopt/optim/radam.py:49\u001b[0m, in \u001b[0;36mRiemannianAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:101\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     93\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     94\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_fn()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:233\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:199\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision_plugin.py:67\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m     tensor: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m        \\**kwargs: Keyword arguments for the same purpose as ``*args``.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1054\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fabric\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1054\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venvs/base/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "50d0b6fb-0aab-43ad-8694-5f04b130a3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1327.6856,   -619.8574,  -1169.5585,   -951.0880,  -1444.6864,\n",
       "         -1327.6856,   -454.4581,   -698.6418,  -1590.7139,  -1490.3971,\n",
       "          -921.1452,   -619.8574,   -454.4581,   -578.9538,   -463.1366,\n",
       "          -338.1800,   -332.7604,   -177.5664,  -1169.5585,   -698.6418,\n",
       "          -578.9538,  -1164.9486,  -1405.0359,  -1027.7635,   -258.9241,\n",
       "         -2227.0105,   -951.0880,  -1590.7139,   -463.1366,  -1164.9486,\n",
       "         -1279.5378,   -806.0576,   -322.7680,  -2158.7980,   -414.1183,\n",
       "         -1444.6864,  -1490.3971,   -338.1800,  -1405.0359,  -1279.5378,\n",
       "          -706.1082,   -469.4789,   -925.6813,   -318.5709,   -392.6118,\n",
       "          -921.1452,   -332.7604,  -1027.7635,   -806.0576,   -706.1082,\n",
       "          -437.6573,   -766.5232,   -197.0147,   -148.8135,   -237.3889,\n",
       "          -177.5664,   -258.9241,   -322.7680,   -469.4789,   -437.6573,\n",
       "          -791.6209,   -117.4669,   -135.0781,   -232.8197,   -219.0742,\n",
       "         -2227.0105,  -2158.7980,   -925.6813,   -766.5232,   -791.6209,\n",
       "          -544.4277,   -498.5291,   -421.5596,   -324.3930,  -1830.3342,\n",
       "          -414.1183,   -318.5709,   -197.0147,   -117.4669,   -544.4277,\n",
       "           -59.1550,   -112.9215,   -121.0100,   -233.6669,   -141.2961,\n",
       "          -392.6118,   -148.8135,   -135.0781,   -498.5291,    -59.1550,\n",
       "           -91.0134,    -50.5226,   -219.2440,   -113.0448,   -503.7755,\n",
       "          -237.3889,   -232.8197,   -421.5596,   -112.9215,    -91.0134,\n",
       "           -52.8696,   -222.3589,   -140.0173,   -477.2410,    -73.9200,\n",
       "          -219.0742,   -324.3930,   -121.0100,    -50.5226,    -52.8696,\n",
       "          -273.7679,    -99.3759,   -512.4581,    -54.9692,   -138.5251,\n",
       "         -1830.3342,   -233.6669,   -219.2440,   -222.3589,   -273.7679,\n",
       "          -533.4449,  -1380.2094,   -165.2919,   -435.9680,   -161.9822,\n",
       "          -141.2961,   -113.0448,   -140.0173,    -99.3759,   -533.4449,\n",
       "          -558.7467,    -40.4777,   -132.2800,    -70.5207,   -150.1847,\n",
       "          -503.7755,   -477.2410,   -512.4581,  -1380.2094,   -558.7467,\n",
       "          -313.6030,   -357.8732,   -257.4461,   -457.1737,   -551.6373,\n",
       "           -73.9200,    -54.9692,   -165.2919,    -40.4777,   -313.6030,\n",
       "           -57.9723,    -33.1102,    -98.3442,    -73.5220,   -221.9386,\n",
       "          -138.5251,   -435.9680,   -132.2800,   -357.8732,    -57.9723,\n",
       "           -66.5422,   -177.6333,   -172.1942,   -431.6728,   -333.5361,\n",
       "          -161.9822,    -70.5207,   -257.4461,    -33.1102,    -66.5422,\n",
       "           -71.6511,    -59.0505,   -167.7033,   -133.4438,   -420.5690,\n",
       "          -150.1847,   -457.1737,    -98.3442,   -177.6333,    -71.6511,\n",
       "          -155.2170,   -152.4198,   -308.3634,   -473.7109,    -93.1266,\n",
       "          -551.6373,    -73.5220,   -172.1942,    -59.0505,   -155.2170,\n",
       "          -248.3426,   -198.3320,   -462.8507,    -87.4607,   -419.6928,\n",
       "          -221.9386,   -431.6728,   -167.7033,   -152.4198,   -248.3426,\n",
       "          -285.5651,   -878.9589,   -188.5674,  -1020.5504,   -338.2727,\n",
       "          -333.5361,   -133.4438,   -308.3634,   -198.3320,   -285.5651,\n",
       "          -962.6720,    -80.8752,   -913.3780,   -269.9229,   -191.1059,\n",
       "          -420.5690,   -473.7109,   -462.8507,   -878.9589,   -962.6720,\n",
       "          -306.0060,  -1390.9607,   -497.2707,   -590.9339,   -720.0860,\n",
       "           -93.1266,    -87.4607,   -188.5674,    -80.8752,   -306.0060,\n",
       "          -253.7142,    -62.0751,    -50.4703,   -144.6756,   -582.2841,\n",
       "          -419.6928,  -1020.5504,   -913.3780,  -1390.9607,   -253.7142,\n",
       "          -503.3114,   -364.9321,   -655.5823,  -3277.3752,  -1708.9133,\n",
       "          -338.2727,   -269.9229,   -497.2707,    -62.0751,   -503.3114,\n",
       "          -194.5188,   -145.1200,  -1264.6850,   -897.8465,   -451.7313,\n",
       "          -191.1059,   -590.9339,    -50.4703,   -364.9321,   -194.5188,\n",
       "          -244.7976,   -734.2957,   -587.5277,   -210.6835,   -746.4787,\n",
       "          -720.0860,   -144.6756,   -655.5823,   -145.1200,   -244.7976,\n",
       "         -1712.6729,   -849.6720,   -561.1035,  -2317.3403,    -85.4708,\n",
       "          -582.2841,  -3277.3752,  -1264.6850,   -734.2957,  -1712.6729,\n",
       "         -4087.0993,  -3965.7329, -10523.5176,   -654.9047,  -1340.5903,\n",
       "         -1708.9133,   -897.8465,   -587.5277,   -849.6720,  -4087.0993,\n",
       "         -2436.3085,  -5386.6710,   -354.2280,  -2391.6996,   -712.5563,\n",
       "          -451.7313,   -210.6835,   -561.1035,  -3965.7329,  -2436.3085,\n",
       "         -2537.4511,   -382.1209,  -1339.8568,   -560.6720,   -440.8953,\n",
       "          -746.4787,  -2317.3403, -10523.5176,  -5386.6710,  -2537.4511,\n",
       "         -1382.0478,  -4646.5636,  -1866.9641,  -2048.1396,  -3168.5368,\n",
       "           -85.4708,   -654.9047,   -354.2280,   -382.1209,  -1382.0478,\n",
       "          -196.7806,    -82.5076,    -95.4469,   -191.5333,  -1340.5903,\n",
       "         -2391.6996,  -1339.8568,  -4646.5636,   -196.7806,   -275.8814,\n",
       "          -639.8797,   -394.1796,   -712.5563,   -560.6720,  -1866.9641,\n",
       "           -82.5076,   -275.8814,   -113.1096,   -349.8759,   -440.8953,\n",
       "         -2048.1396,    -95.4469,   -639.8797,   -113.1096,   -645.2128,\n",
       "         -3168.5368,   -191.5333,   -394.1796,   -349.8759,   -645.2128,\n",
       "          -282.8942,   -881.5839,  -1566.4813,   -552.5134,   -420.3493,\n",
       "          -899.6284,   -558.4820,   -279.8963,   -430.5320,  -1116.5573,\n",
       "          -721.5746,   -413.8944,   -124.8035,   -170.2665,   -822.5843,\n",
       "          -157.0093,   -116.9452,   -183.5958,   -395.1375,   -519.5265,\n",
       "          -588.0465,   -644.5892,   -256.8949,   -723.1820,    -87.1658,\n",
       "          -151.2826,   -661.5295,   -624.6993,   -191.4536,   -763.2304,\n",
       "         -2106.5611,   -123.6870,  -1416.2038,   -768.9376,   -783.3253,\n",
       "          -175.5117,   -166.4294,  -3439.9065,    -86.5980,   -478.6753,\n",
       "         -2521.7475,  -1005.2987,   -408.6253,   -242.1256,   -323.3493,\n",
       "           -96.0281,   -245.8828,  -1267.0655,   -675.7718,  -1093.9205,\n",
       "           -75.0890,   -656.3463,   -820.4706,    -44.8790,   -256.9937,\n",
       "          -116.4177,   -124.3399,   -691.2218,   -647.4082,  -2744.1763,\n",
       "          -118.5096,  -3613.3654,   -142.9723,   -364.3504,   -219.0967,\n",
       "           -46.9630,   -243.8224,   -976.2487,  -1703.8433,   -165.5195,\n",
       "         -1238.3849,   -424.3873,  -2180.0303,    -44.6422,   -147.6563,\n",
       "         -1515.5948,   -345.6045,  -1317.3329,  -1066.6054,  -1394.5970,\n",
       "         -8507.3467,   -487.2499,   -374.1667,   -506.8534,   -396.8160,\n",
       "          -985.9341,   -201.8775,  -1075.6049,   -520.2506,   -207.4444,\n",
       "          -219.5982,   -327.0367,   -764.7479,    -82.8096,   -303.9618,\n",
       "          -953.0215,   -248.1751,   -363.1867,   -173.2985,   -304.7688,\n",
       "          -117.8020,   -372.1649,   -181.8788,   -256.2378,   -477.8662,\n",
       "          -241.9063,  -1383.4581,   -264.6403,   -924.0182,   -276.1851,\n",
       "          -233.0038,    -40.8193,   -221.3043,  -4396.8417,   -120.1985,\n",
       "          -566.6025,  -1129.1108,   -533.9343,  -1076.3995,   -443.7577,\n",
       "          -100.0956,   -175.0128,   -450.6398,  -1531.8221,   -107.2534,\n",
       "          -480.9503,   -707.3829,   -316.6807,   -678.9261,   -151.3579,\n",
       "          -113.4675,    -42.5838,   -482.3442,   -140.3583,   -214.8200,\n",
       "         -3384.4451,   -161.6701,   -538.2384,   -235.0553,   -318.2120,\n",
       "          -194.5774,   -590.2802,   -110.6434,   -458.4347,   -377.0356,\n",
       "          -174.6215,   -484.2446,   -160.8281,   -388.1262,   -354.6384,\n",
       "          -145.0913,  -4959.9798,    -91.9729,  -1293.5585,  -4006.8886,\n",
       "         -1189.0381,  -1113.2549,   -463.1659,   -522.3576,   -183.6714,\n",
       "          -387.1705,   -761.5262,  -1041.3448,  -1084.3698,   -111.7355,\n",
       "          -330.1943,   -309.6642,    -95.9456,   -458.5415,   -189.2109,\n",
       "          -110.9120,   -387.4664,   -755.3520,   -255.7604,   -129.7703,\n",
       "          -801.3234,   -119.2497,   -167.4869,   -420.3439,    -93.8346,\n",
       "          -170.4786,   -360.1601,   -195.6110,   -431.7366,   -645.8159,\n",
       "          -468.7497,   -885.0825,   -107.9740,   -111.0916, -30762.9806,\n",
       "          -295.0592,  -1973.0000,   -697.3485,  -1481.6708,  -4153.1308,\n",
       "           -35.8356,   -824.7335,   -451.1328,    -73.6334,   -237.7676,\n",
       "         -1051.7217,   -101.6651,   -649.9019,   -604.3857,   -277.8160,\n",
       "          -140.5743,   -687.9232,    -76.5192,   -197.2852,   -736.2145,\n",
       "          -508.5200,  -1206.6232,   -238.9848,   -366.8084,   -312.3272,\n",
       "          -151.4888,   -152.6216,    -76.2122,    -94.7470,   -645.1825,\n",
       "          -598.2070,   -817.2137,   -263.9001,   -574.9022,   -660.9027,\n",
       "          -144.6038,   -192.8733,   -779.1027,   -741.3640,   -988.7274,\n",
       "          -704.6068,   -451.9333,  -1490.1281,   -618.1715,   -253.7228,\n",
       "           -73.8522,   -352.9873,  -1025.5532,   -604.0886,   -567.4822,\n",
       "          -511.8593,   -407.1309,  -1444.4782,   -377.3203,   -144.7305,\n",
       "           -95.3007,    -89.2261,   -770.0291,   -257.3960,  -1133.4576,\n",
       "          -127.7581,   -268.0848,   -215.6703,   -240.8378,   -128.5961,\n",
       "          -132.3903,   -218.8783,   -145.3276,   -769.4019,   -224.8359,\n",
       "         -1644.6163,   -182.8735,   -675.9617,    -77.9091,   -383.5224,\n",
       "         -5650.7707,   -155.8950,   -821.2053,   -269.3111,   -221.2413,\n",
       "         -8026.6650,   -153.6683,   -217.7352,   -470.8170,   -177.0506,\n",
       "          -749.3631,   -719.8156,   -689.6284,   -422.6884,   -182.8647,\n",
       "          -781.3689,    -47.5800,   -367.7478,   -101.6797,   -178.1109,\n",
       "          -982.6631,   -483.8792,  -2319.6040,   -187.2225,   -166.0483,\n",
       "          -276.1987,   -147.4997,   -100.0253,  -1508.6424,  -1299.5593,\n",
       "          -340.0762,   -428.8482,   -150.0280,   -692.3359,   -340.8322,\n",
       "          -603.5864,   -101.5410,   -113.4109,  -1026.6881,    -72.1890,\n",
       "         -1467.6234,  -3711.6187,   -369.8308,  -2365.6027,   -613.1093,\n",
       "          -590.0256,   -799.7846,   -224.9772,  -1042.4706,  -1916.5702,\n",
       "          -691.7918,   -210.9957,   -203.3732,   -672.1089,   -163.2207,\n",
       "          -184.7563,   -345.5989,   -244.8647,   -827.0608,   -219.5094,\n",
       "         -1710.1048,   -114.3172,   -684.1296,   -357.3996,   -528.1049,\n",
       "          -550.4428,   -106.6939,   -452.8061,   -349.1004,   -199.8933,\n",
       "          -168.3893,   -439.6873,   -221.1450,  -1178.3814,    -52.6858,\n",
       "          -147.9508,  -2071.0901,    -81.9644,  -4272.6970,  -1125.8298,\n",
       "          -608.7871,   -429.2701,   -847.2093,   -142.2700,   -187.7667,\n",
       "           -82.7588,   -555.4444,  -4632.4705,   -340.9513,   -102.4321,\n",
       "           -61.8873,  -1098.8922,   -302.5304,    -99.4233,    -40.5716,\n",
       "          -328.3114,   -437.7652,   -445.9749,   -676.4675,   -161.6252,\n",
       "          -137.5526,   -189.6247,   -428.1272,   -177.5355,   -362.1364,\n",
       "           -91.3721,   -611.5138,   -261.2020,   -304.4207,  -2755.7748,\n",
       "         -1212.0432,   -891.2874,    -79.4666,    -56.1943,   -996.4035],\n",
       "       dtype=torch.float64, grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dataset[5]\n",
    "z = model.embd(x)\n",
    "z1, z2 = z[:, 0], z[:, 1]\n",
    "scores = model.sg(z1, z2)\n",
    "#model.loss_fn(scores, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8cc39-9345-4394-b59a-a440c6001ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
