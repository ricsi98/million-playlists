{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53398f7d-50bc-4667-bfdb-eeaf7b4b24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        ntoken, d_model = embeddings.shape\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken, bias=embeddings is None)\n",
    "\n",
    "        self.init_weights(embeddings)\n",
    "\n",
    "    def init_weights(self, embeddings) -> None:\n",
    "        initrange = 0.1\n",
    "        if embeddings is None:\n",
    "            self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "            self.decoder.bias.data.zero_()\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        else:\n",
    "            self.encoder.weight.data.copy_(embeddings)\n",
    "            self.decoder.weight.data.copy_(embeddings)\n",
    "            self.encoder.weight.requires_grad = False\n",
    "            self.decoder.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f84c1b-27c0-436f-b9f5-aac0edbafd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33db5a-de73-4cfb-8bc1-366dd9af5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "class PlaylistDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, files, playlist_per_file, transform=None):\n",
    "        self.files = files\n",
    "        self.current_file_index = -1\n",
    "        self.data = None\n",
    "        self.ppf = playlist_per_file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.ppf * len(self.files)\n",
    "    \n",
    "    def _load(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_index = index // self.ppf\n",
    "        offset = index % self.ppf\n",
    "        if self.current_file_index != file_index:\n",
    "            logging.debug(f\"Loading file {self.files[file_index]}\")\n",
    "            self._load(self.files[file_index])\n",
    "            self.current_file_index = file_index\n",
    "        tracks = self.data[\"playlists\"][offset]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            tracks = self.transform(tracks)\n",
    "        \n",
    "        return tracks\n",
    "\n",
    "    \n",
    "class Compose:\n",
    "    \n",
    "    def __init__(self, *tfs):\n",
    "        self.tfs = tfs\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for tf in self.tfs:\n",
    "            x = tf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RemoveUnknownTracks:\n",
    "    \n",
    "    def __init__(self, known_tracks):\n",
    "        kt = known_tracks\n",
    "        if not isinstance(kt, set):\n",
    "            kt = set(kt)\n",
    "        self.kt = kt\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [xi for xi in x if xi in self.kt]\n",
    "    \n",
    "    \n",
    "class TrackURI2Idx:\n",
    "    \n",
    "    def __init__(self, uri2idx, offset=0):\n",
    "        self.offset = offset\n",
    "        self.uri2idx = uri2idx\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [self.uri2idx[xi] + self.offset for xi in x]\n",
    "    \n",
    "    \n",
    "class ToLongTensor:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return torch.LongTensor(x)\n",
    "    \n",
    "class PadOrTrim:\n",
    "    \n",
    "    def __init__(self, pad_token, target_length):\n",
    "        self.token = pad_token\n",
    "        self.t = target_length\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if len(x) == self.t:\n",
    "            return x\n",
    "        if len(x) < self.t:\n",
    "            return x + [self.token] * (self.t - len(x))\n",
    "        return x[:self.t]\n",
    "    \n",
    "\"\"\"\n",
    "class MaskTracksTensor:\n",
    "    \n",
    "    def __init__(self, mask_token, padding_token, mask_proba):\n",
    "        self.token = mask_token\n",
    "        self.padding_token = padding_token\n",
    "        self.proba = mask_proba\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        mask = torch.rand(x.shape[0]) < self.proba\n",
    "        padding = x == self.padding_token\n",
    "        # avoid masking padded tracks\n",
    "        mask = mask & (~padding)\n",
    "        x_ = x.clone()\n",
    "        x_[mask] = self.token\n",
    "        return x_, x, mask\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1f9d-2c9f-4bd2-a73a-04465e608cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    seq_len = source.shape[1]-1\n",
    "    data = source[:, :seq_len]\n",
    "    target = source[:, 1:]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23d5a9-6284-4ece-8b52-649c0831c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = Word2Vec.load(\"../checkpoints/model_final.model\").wv\n",
    "song2vec = torch.tensor(wv.vectors)\n",
    "song2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cb7b5-0b59-4b53-b656-e940859235ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"../playlists_data/chunk_{i}.json\" for i in range(20)]\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "MASK_TOKEN = 1\n",
    "SEQ_LEN = 15\n",
    "\n",
    "transforms = Compose(\n",
    "    RemoveUnknownTracks(wv.key_to_index.keys()),\n",
    "    TrackURI2Idx(wv.key_to_index, offset=1),\n",
    "    PadOrTrim(PAD_TOKEN, SEQ_LEN),\n",
    "    ToLongTensor(),\n",
    ")\n",
    "\n",
    "ds = PlaylistDataset(files, 50000, transforms)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b7519-210a-4bc0-8723-4366e48fa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "ntokens = len(song2vec)  # size of vocabulary\n",
    "emsize = song2vec.shape[1]  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(song2vec, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9385715-4af9-4c9c-803b-e46d7296c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module, loader) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(32).to(device)\n",
    "\n",
    "    for batch in loader:\n",
    "        data, targets = get_batch(batch)\n",
    "        seq_len = data.size(0)\n",
    "        print(data.shape)\n",
    "        output = model(data, src_mask)\n",
    "        print(output.shape, targets.shape)\n",
    "        loss = criterion(output.view(-1, len(song2vec)), targets.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "        cur_loss = total_loss / log_interval\n",
    "        ppl = math.exp(cur_loss)\n",
    "        print(f'| epoch {epoch:3d} | '\n",
    "              f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "              f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d504b-29ba-454d-89bf-1686f3961866",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, dl)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237d1e8-8afd-4863-a085-ccc49ed885a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
