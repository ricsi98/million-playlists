{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd209af1-1047-49da-a636-5c52af4bc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f606fca-c451-400f-b602-9fedf0dedbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1_000_000\n",
    "w = np.random.uniform(-3,3, size=(N,))\n",
    "w = np.exp(w) / np.sum(np.exp(w))\n",
    "z = list(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54933c1e-6474-46da-aa34-58a0b6042273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.random.choice(z, 1024, p=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421855e-efc8-4e64-a1b6-c6c1d08c889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "cw = np.cumsum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b41cc-27d5-4563-a144-c84ffb4a4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.array(random.choices(z, cum_weights=cw, k=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf472f-ed37-443f-a083-e58cd2d2bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf43ce-5c8a-4a16-bebd-aa0747048968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferedH5Writer:\n",
    "\n",
    "    def __init__(self, file, dataset, buffer_size) -> None:\n",
    "        self.file = file\n",
    "        self.dataset = dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def _len(self):\n",
    "        if len(self.buffer) == 0:\n",
    "            return 0\n",
    "        return sum(len(xi) for xi in self.buffer)\n",
    "        \n",
    "    def _write(self):\n",
    "        if self._len() == 0:\n",
    "            return\n",
    "        h, w = self.file[self.dataset].shape\n",
    "        n = self._len()\n",
    "        if n > h - self.index:\n",
    "            self.file[self.dataset].resize((self.index + n, w))\n",
    "        data = np.concatenate((self.buffer), axis=0).reshape(n, w)\n",
    "        self.file[self.dataset][self.index:self.index+n,:] = data\n",
    "        self.index += n\n",
    "        self.file.flush()\n",
    "        self.buffer = []\n",
    "\n",
    "    def append(self, data: np.ndarray):\n",
    "        self.buffer.append(data)\n",
    "        if self._len() >= self.buffer_size:\n",
    "            self._write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075bb916-8816-4a55-8714-e6d3a354ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"test.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"ds\", (5,2), maxshape=(None, 2))\n",
    "    bw = BufferedH5Writer(f, \"ds\", 3)\n",
    "    for i in range(10):\n",
    "        z = np.array([[1, 1]] * 4) * i\n",
    "        bw.append(z)\n",
    "    bw._write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322bdf9-1bce-4d5a-8cc3-a75abf557aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"../data/kcore/skipgrams.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d588499-98f6-4df6-8cb9-0cf461396f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "randids = np.sort(np.random.randint(0, len(f[\"data\"]), size=(2048,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044100f4-e77c-4ecc-a972-3b9519c54eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f[\"data\"][:50000000]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cf095-d591-4bc0-b304-4f5f7d117c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "randids = np.sort(np.random.randint(0, 50000000, size=(2048,)))\n",
    "f[\"data\"][randids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedddf2-d4be-453e-b84a-cc0e43d41ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "randids = np.sort(np.random.randint(0, 50000000, size=(2048,)))\n",
    "data[randids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e9db1-b298-4146-b7ee-0ce604591f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2048\n",
    "arr = np.zeros((BS, 2))\n",
    "with h5py.File(\"../data/kcore/skipgrams.h5\", \"r\") as f:\n",
    "    l = len(f[\"data\"])\n",
    "    for i in range(0, l, BS):\n",
    "        arr[:] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb28ab-a0f5-453b-9138-cf08d1cdf7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"../data/kcore/skipgrams.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfd49c-2595-4447-a899-259722e5f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "z = np.random.randint(0, len(f[\"data\"]))\n",
    "f[\"data\"][z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b2989-16b7-44a0-af59-63ffa3864f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = H5Dataset(\"../data/kcore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2192d29-3c66-4f36-89db-52b47fd2c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27b7967-65fa-454e-ab37-ffe406e1dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358451802\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../data/kcore/skipgrams.h5\", \"r\") as f:\n",
    "    print(len(f[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60141238-0e90-43e6-b16e-fda3b6980adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e98fe1e9-f46c-4b66-a50a-9f6c69904259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "\n",
    "\n",
    "class constants:\n",
    "    FNAME_FREQUENCIES = \"frequencies.json\"\n",
    "    FNAME_SKIPGRAMS = \"skipgrams.h5\"\n",
    "    FNAME_IDX2SONG = \"idx2song.json\"\n",
    "class H5Dataset(Dataset):\n",
    "\n",
    "    DEFAULT_BATCH_SIZE = 512\n",
    "    DEFAULT_BUFFER_SIZE = 512 * 4096 * 4\n",
    "    \n",
    "\n",
    "    def __init__(self, \n",
    "                 path: str, \n",
    "                 alpha: float = 0.75, \n",
    "                 n_negatives: int = 1, \n",
    "                 buffer_size: int = DEFAULT_BUFFER_SIZE, \n",
    "                 batch_size: int = DEFAULT_BATCH_SIZE,\n",
    "                 device: str = \"cpu\"\n",
    "        ):\n",
    "        self.path = path\n",
    "        self.n_negatives = n_negatives\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self._init_negative_distribution(path, alpha)\n",
    "        self.__data = {\n",
    "            \"chunk_index\": -1,\n",
    "            \"cap\": buffer_size,\n",
    "            \"data\": torch.zeros((buffer_size, 2), dtype=torch.long).to(self.device)\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def _batch_per_chunk(self):\n",
    "        return math.ceil(self.buffer_size / self.batch_size)\n",
    "    \n",
    "    @property\n",
    "    def _n_chunks(self):\n",
    "        if not hasattr(self, \"__n_chunks\"):\n",
    "            path = os.path.join(self.path, constants.FNAME_SKIPGRAMS)\n",
    "            with h5py.File(path, \"r\") as f:\n",
    "                self.__n_chunks = math.ceil(len(f[\"data\"]) / self.buffer_size)\n",
    "        return self.__n_chunks \n",
    "    \n",
    "    def __len__(self):\n",
    "        if not hasattr(self, \"__n_batches\"):\n",
    "            path = os.path.join(self.path, constants.FNAME_SKIPGRAMS)\n",
    "            with h5py.File(path, \"r\") as f:\n",
    "                self.__n_batches = math.ceil(len(f[\"data\"]) / self.batch_size)\n",
    "        return self.__n_batches\n",
    "        \n",
    "    def _init_negative_distribution(self, path: str, alpha: float):\n",
    "        with open(os.path.join(path, constants.FNAME_FREQUENCIES), \"r\") as f:\n",
    "            freqs = json.load(f)\n",
    "            weights = np.array(list(freqs.values())) ** alpha\n",
    "            weights /= np.sum(weights)\n",
    "            self.cum_weights = np.cumsum(weights)\n",
    "            self.ids = np.arange(len(weights), dtype=np.int64)\n",
    "\n",
    "    def _get(self, batch_index):\n",
    "        index_start = batch_index * self.batch_size\n",
    "        chunk_index = index_start // self.buffer_size\n",
    "        if chunk_index != self.__data[\"chunk_index\"]:\n",
    "            logging.info(f\"Loading chunk {chunk_index}\")\n",
    "            path = os.path.join(self.path, constants.FNAME_SKIPGRAMS)\n",
    "            with h5py.File(path, \"r\") as f:\n",
    "                start_ = chunk_index * self.buffer_size \n",
    "                end_ = (chunk_index + 1) * self.buffer_size\n",
    "                assert start_ < len(f[\"data\"]), \"index out of bounds\"\n",
    "                length = min(self.buffer_size, len(f[\"data\"]) - start_)\n",
    "                buffer = torch.empty((length, 2))\n",
    "                f[\"data\"].read_direct(buffer.numpy(), source_sel=np.s_[start_:end_])\n",
    "                self.__data[\"data\"][:length] = buffer\n",
    "                self.__data[\"chunk_index\"] = chunk_index\n",
    "                self.__data[\"cap\"] = length\n",
    "        offset = index_start - chunk_index * self.buffer_size\n",
    "        return self.__data[\"data\"][min(self.__data[\"cap\"], offset):min(self.__data[\"cap\"], offset+self.batch_size)]    \n",
    "\n",
    "    def _negatives(self, k):\n",
    "        return torch.tensor(random.choices(self.ids, cum_weights=self.cum_weights, k=k))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pos = self._get(index)\n",
    "        neg = self._negatives(len(pos) * self.n_negatives)\n",
    "        neg = torch.stack((pos[:, 0], neg), dim=1).to(pos)\n",
    "        x = torch.cat((pos, neg), dim=0)\n",
    "        y = torch.tensor([1] * len(pos) + [0] * len(neg))\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "\n",
    "class RandomOrderSampler(Sampler[int]):\n",
    "\n",
    "    def __init__(self, from_: int, to_: int, generator=None) -> None:\n",
    "        assert to_ > from_\n",
    "        self.from_ = from_\n",
    "        self.to_ = to_\n",
    "        self.n_ = to_ - from_\n",
    "        self.generator = generator\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in torch.randperm(self.n_, generator=self.generator):\n",
    "            yield self.from_ + i\n",
    "    \n",
    "\n",
    "class CustomH5Sampler(Sampler):\n",
    "    \"\"\"Sample indices within loaded chunk then skip to next chunk\"\"\"\n",
    "\n",
    "    def __init__(self, data_source, generator=None) -> None:\n",
    "        super().__init__(data_source)\n",
    "        assert isinstance(data_source, H5Dataset)\n",
    "        self.ds = data_source\n",
    "        self.generator = generator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        chunk_sampler = RandomOrderSampler(0, self.ds._n_chunks, self.generator)\n",
    "        for chunk_index in chunk_sampler:\n",
    "            chunk_index = chunk_index.item()\n",
    "            start_ = math.floor(chunk_index * self.ds.buffer_size / self.ds.batch_size)\n",
    "            end_ = min(start_ + self.ds._batch_per_chunk, len(self.ds))\n",
    "            sampler = RandomOrderSampler(start_, end_, self.generator)\n",
    "            for i in sampler:\n",
    "                yield i.item()\n",
    "\n",
    "\n",
    "def _collate_fn(sample):\n",
    "    x, y = list(zip(*sample))\n",
    "    return torch.cat(x, dim=0), torch.cat(y, dim=0)\n",
    "\n",
    "\n",
    "def get_data_loader(\n",
    "        path: str, \n",
    "        alpha: float = 0.75, \n",
    "        n_negatives: int = 1, \n",
    "        buffer_size: int = H5Dataset.DEFAULT_BUFFER_SIZE,\n",
    "        dataset_batch_size: int = H5Dataset.DEFAULT_BATCH_SIZE,\n",
    "        **loader_kwargs\n",
    "    ):\n",
    "    ds = H5Dataset(\n",
    "        path=path,\n",
    "        alpha=alpha,\n",
    "        n_negatives=n_negatives,\n",
    "        buffer_size=buffer_size\n",
    "    )\n",
    "    sampler = CustomH5Sampler(ds)\n",
    "    return DataLoader(\n",
    "        dataset=ds,\n",
    "        sampler=sampler,\n",
    "        collate_fn=_collate_fn,\n",
    "        **loader_kwargs\n",
    "    ), ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d0694750-67fb-49c1-b5f3-b26d7e293416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = H5Dataset(\"../data/kcore/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7ec3f5ad-ac34-4254-987f-54e92b572337",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 700102/700102 [00:14<00:00, 49558.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "358451802"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "for i in tqdm(range(len(ds)), total=len(ds)):\n",
    "    x = ds[i]\n",
    "    n += len(x)\n",
    "    assert len(x) > 0\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e9d381b8-50b0-4f44-864e-6e4f77827127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "358451802 - n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3a17119c-02dd-4c96-8fc1-f3ab987a0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2a8fb0c4-a4ee-4b9c-8a73-f589130d50f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87513\n"
     ]
    }
   ],
   "source": [
    "loader, ds = get_data_loader(\"../data/kcore/\", batch_size=8)\n",
    "print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "04727f44-6da1-489e-b342-1dc200ca436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/87513 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 2]) torch.Size([8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(b)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, b[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43masd\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(n)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for b in tqdm(loader):\n",
    "    n += len(b)\n",
    "    print(b[0].shape, b[1].shape)\n",
    "    asd\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5e36267c-4c7f-43d3-913e-3d9cb33f61c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n - 358451802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4e040394-364f-4058-88c5-397ed901249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ds.x)) - len(ds.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aaddef-266f-4277-a25d-e57e4c02f3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%prun\n",
    "\n",
    "for i, b in enumerate(loader):\n",
    "    if i > 100:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54389439-630b-447e-ab03-602004b07ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51444c8-85e0-412f-80cf-acfc006848be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
