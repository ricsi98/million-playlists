{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43aa9099-6d18-4bcc-bed1-d0d6414032ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder, Embedding\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e82888-c488-4b72-9b13-68c3ff7f56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec.load(\"../checkpoints/model_final.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80341712-c11f-4d8e-83c9-27d4b48559e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer from gensim embeddings\n",
    "print(wv.vector_size, len(wv))\n",
    "embd = Embedding(len(wv), wv.vector_size)\n",
    "embd.weight.data.copy_(torch.tensor(wv.vectors, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9c7ca-6488-402a-b9b6-6e1486aafbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(wv.vector_size, 10, batch_first=True)\n",
    "encoder = TransformerEncoder(encoder_layer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244aeea-7524-47da-b7a3-a7ed9fd22dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = random.choices(list(wv.key_to_index.keys()), k=12)\n",
    "t_seq = torch.tensor([wv.key_to_index[item] for item in seq], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6b777-e40d-484f-98b8-be3316023744",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = embd(t_seq.view(1,-1))\n",
    "encoder(v).shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af6187-1540-49a8-9d0c-bd81261cccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28195e-c0c7-4109-bc7f-bc6830a81a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask padding: src_key_padding_mask\n",
    "# mask some items: mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0185c7-1391-436a-b00b-cf7f85f3da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = batch[2].shape\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5caf656-addf-41f5-beff-abdc180e5630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _generate_attn_mask_single(seq_mask):\n",
    "    \"\"\"\n",
    "        If a BoolTensor is provided, positions with True are not allowed \n",
    "        to attend while False values will be unchanged.\n",
    "        Softmax goes along -1 dimension\n",
    "    \"\"\"\n",
    "    n = seq_mask.shape[0]\n",
    "    mask = torch.zeros((n,n), dtype=torch.bool)\n",
    "    mask[:, seq_mask.nonzero()] = True\n",
    "    return mask\n",
    "\n",
    "def _generate_attn_mask_batch(seq_mask, n_heads):\n",
    "    bs, n = seq_mask.shape\n",
    "    mask = torch.zeros((bs, n, n), dtype=torch.bool)\n",
    "    nz = seq_mask.nonzero()\n",
    "    a, b = nz[:, 0], nz[:, 1]\n",
    "    mask[a, :, b] = True\n",
    "    if n_heads > 1:\n",
    "        mask = mask.repeat(1, n_heads, 1)\n",
    "        mask = mask.view(bs * n_heads, n, n)\n",
    "    return mask\n",
    "\n",
    "def generate_attn_mask(seq_mask, n_heads=1):\n",
    "    if len(seq_mask.shape) == 1:\n",
    "        return _generate_attn_mask_single(seq_mask, n_heads)\n",
    "    elif len(seq_mask.shape) == 2:\n",
    "        return _generate_attn_mask_batch(seq_mask, n_heads)\n",
    "    else:\n",
    "        assert False, f\"Input should be BATCH_SIZE * SEQ_LEN matrix, got {seq_mask.shape}\"\n",
    "            \n",
    "mask = generate_attn_mask(torch.Tensor([[False, False, True, False, True], [False, False, False, False, True]]))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bdf66-19a7-4f38-935a-6e68f93343de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.Tensor([False] * v.shape[1])\n",
    "attn_mask = generate_attn_mask(mask).repeat(10,1,1)\n",
    "attn_mask.shape, v.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d36ab2-1f13-41d6-a491-919ee16c196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b0f2e-8a17-465e-aea2-529bf2da94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encoder(v, mask=attn_mask)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d4c616-62ca-43a4-8011-fa48e10333d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "class PlaylistDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, files, playlist_per_file, transform=None):\n",
    "        self.files = files\n",
    "        self.current_file_index = -1\n",
    "        self.data = None\n",
    "        self.ppf = playlist_per_file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.ppf * len(self.files)\n",
    "    \n",
    "    def _load(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_index = index // self.ppf\n",
    "        offset = index % self.ppf\n",
    "        if self.current_file_index != file_index:\n",
    "            logging.debug(f\"Loading file {self.files[file_index]}\")\n",
    "            self._load(self.files[file_index])\n",
    "            self.current_file_index = file_index\n",
    "        tracks = self.data[\"playlists\"][offset]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            tracks = self.transform(tracks)\n",
    "        \n",
    "        return tracks\n",
    "\n",
    "    \n",
    "class Compose:\n",
    "    \n",
    "    def __init__(self, *tfs):\n",
    "        self.tfs = tfs\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for tf in self.tfs:\n",
    "            x = tf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RemoveUnknownTracks:\n",
    "    \n",
    "    def __init__(self, known_tracks):\n",
    "        kt = known_tracks\n",
    "        if not isinstance(kt, set):\n",
    "            kt = set(kt)\n",
    "        self.kt = kt\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [xi for xi in x if xi in self.kt]\n",
    "    \n",
    "    \n",
    "class TrackURI2Idx:\n",
    "    \n",
    "    def __init__(self, uri2idx, offset=0):\n",
    "        self.offset = offset\n",
    "        self.uri2idx = uri2idx\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [self.uri2idx[xi] + self.offset for xi in x]\n",
    "    \n",
    "    \n",
    "class ToLongTensor:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return torch.LongTensor(x)\n",
    "    \n",
    "class PadOrTrim:\n",
    "    \n",
    "    def __init__(self, pad_token, target_length):\n",
    "        self.token = pad_token\n",
    "        self.t = target_length\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if len(x) == self.t:\n",
    "            return x\n",
    "        if len(x) < self.t:\n",
    "            return x + [self.token] * (self.t - len(x))\n",
    "        return x[:self.t]\n",
    "    \n",
    "    \n",
    "class MaskTracksTensor:\n",
    "    \n",
    "    def __init__(self, mask_token, padding_token, mask_proba):\n",
    "        self.token = mask_token\n",
    "        self.padding_token = padding_token\n",
    "        self.proba = mask_proba\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        mask = torch.rand(x.shape[0]) < self.proba\n",
    "        padding = x == self.padding_token\n",
    "        # avoid masking padded tracks\n",
    "        mask = mask & (~padding)\n",
    "        x_ = x.clone()\n",
    "        x_[mask] = self.token\n",
    "        return x_, x, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d793f-9e3f-4299-8b8d-8984c624c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.BoolTensor([True, False, False, True])\n",
    "pad = torch.BoolTensor([False, False, True, True])\n",
    "\n",
    "mask & (~pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439e89b4-6aaf-4f77-8cae-7336214af09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../playlists_data/chunk_0.json',\n",
       " '../playlists_data/chunk_1.json',\n",
       " '../playlists_data/chunk_2.json',\n",
       " '../playlists_data/chunk_3.json',\n",
       " '../playlists_data/chunk_4.json',\n",
       " '../playlists_data/chunk_5.json',\n",
       " '../playlists_data/chunk_6.json',\n",
       " '../playlists_data/chunk_7.json',\n",
       " '../playlists_data/chunk_8.json',\n",
       " '../playlists_data/chunk_9.json',\n",
       " '../playlists_data/chunk_10.json',\n",
       " '../playlists_data/chunk_11.json',\n",
       " '../playlists_data/chunk_12.json',\n",
       " '../playlists_data/chunk_13.json',\n",
       " '../playlists_data/chunk_14.json',\n",
       " '../playlists_data/chunk_15.json',\n",
       " '../playlists_data/chunk_16.json',\n",
       " '../playlists_data/chunk_17.json',\n",
       " '../playlists_data/chunk_18.json',\n",
       " '../playlists_data/chunk_19.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f\"../playlists_data/chunk_{i}.json\" for i in range(20)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ec579d-0321-4bab-a02a-8b0f5db81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "MASK_TOKEN = 1\n",
    "\n",
    "transforms = Compose(\n",
    "    RemoveUnknownTracks(wv.key_to_index.keys()),\n",
    "    TrackURI2Idx(wv.key_to_index, offset=2),\n",
    "    PadOrTrim(PAD_TOKEN, 5),\n",
    "    ToLongTensor(),\n",
    "    MaskTracksTensor(MASK_TOKEN, PAD_TOKEN, .1)\n",
    ")\n",
    "\n",
    "ds = PlaylistDataset(files, 50000, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f099e0f-aa9a-404b-856b-49fc884a775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a7bee8b-d5e1-497f-9856-fdee8cd0a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea83a57-d0db-4b7a-8e52-9748dec9ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1857801-d43f-4651-b7ec-28bf8c01a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ec918-0d54-461f-9212-cfa280b26cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cf543-42a7-48f8-9274-a3d809807dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5680137-544f-40bd-850e-a04cc37cdccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "# TODO: add final linear layer\n",
    "\n",
    "GELU = nn.GELU()\n",
    "\n",
    "class TransRec(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, wv_model, n_head, layer_kwargs={}, enc_kwargs={}):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embd = Embedding(len(wv_model)+2, wv_model.vector_size) # +2 for <PAD> and <MASK> tokens\n",
    "        self.embd.weight.data[2:].copy_(torch.tensor(wv_model.vectors, dtype=torch.float))\n",
    "        self.embd.requires_grad_ = False\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(wv_model.vector_size, batch_first=True, \\\n",
    "                                                nhead=n_head, **layer_kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.encoder = TransformerEncoder(encoder_layer, **enc_kwargs)\n",
    "        self.linear = nn.Linear(wv_model.vector_size, wv_model.vector_size, bias=True)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embd(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return GELU(self.linear(x))\n",
    "    \n",
    "    def _token_probs(self, x, mask):\n",
    "        bs, seq_len, embd_dim = x.shape\n",
    "        num_tokens = self.embd.weight.shape[0]\n",
    "        x = x[mask, :]\n",
    "        logits = torch.matmul(self.embd.weight, x.view(-1, embd_dim).T).view(num_tokens, -1)\n",
    "        return logits.softmax(dim=0)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, mask = batch\n",
    "        padding_mask = x == PAD_TOKEN\n",
    "        attn_mask = generate_attn_mask(mask, n_heads=self.n_head)\n",
    "        \n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        y_ = self._token_probs(self.forward(x, mask=attn_mask), mask)\n",
    "        loss = crit(y_.T, y[mask])\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e27f9-d89e-4a5a-aa1f-021b0ac6c5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0ea6efc-a4f2-4e98-a230-b207d5776d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = TransRec(wv, n_head=10, enc_kwargs={\"num_layers\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2cdb434-6377-4a3e-b769-a4c221e55bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e5e11-9b73-4fac-9e34-c9f3f6c33bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | embd    | Embedding          | 28.1 M\n",
      "1 | encoder | TransformerEncoder | 1.4 M \n",
      "2 | linear  | Linear             | 10.1 K\n",
      "-----------------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.959   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                      | 2/1954 [00:05<1:31:49,  2.82s/it, v_num=7]"
     ]
    }
   ],
   "source": [
    "trainer.fit(tr, train_dataloaders=dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c611c7bd-3419-4b83-8187-97695df9fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10405103 / 281219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938f1af-14af-4787-bfbd-ddbd98494814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, mask = batch\n",
    "padding_mask = x == PAD_TOKEN\n",
    "attn_mask = generate_attn_mask(mask, n_heads=tr.n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e90323-56c7-4d66-8533-dba3107dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[x == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15da44-b7d3-4aa3-8d57-9612fc59482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aebf35-7f51-4faf-9dea-26bbc1d87c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tr(x, mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1b80c-761a-4eb4-b9d0-7cd9b855f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:, x==1].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc5379-90ab-42ab-9010-a421b5748d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.argmax(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7c074-fdf4-42a4-825e-c2eb600ad5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.softmax(dim=0).sum(dim=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1506f-6a5b-42eb-a030-60c1d3a95df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
