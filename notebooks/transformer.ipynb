{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43aa9099-6d18-4bcc-bed1-d0d6414032ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoderLayer, TransformerEncoder, Embedding\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64733f2-e415-4dfa-b0ba-af4b2b094114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e82888-c488-4b72-9b13-68c3ff7f56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = Word2Vec.load(\"../checkpoints/model_final.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5caf656-addf-41f5-beff-abdc180e5630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True]],\n",
       "\n",
       "        [[False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _generate_attn_mask_single(seq_mask):\n",
    "    \"\"\"\n",
    "        If a BoolTensor is provided, positions with True are not allowed \n",
    "        to attend while False values will be unchanged.\n",
    "        Softmax goes along -1 dimension\n",
    "    \"\"\"\n",
    "    n = seq_mask.shape[0]\n",
    "    mask = torch.zeros((n,n), dtype=torch.bool)\n",
    "    mask[:, seq_mask.nonzero()] = True\n",
    "    return mask\n",
    "\n",
    "def _generate_attn_mask_batch(seq_mask, n_heads):\n",
    "    bs, n = seq_mask.shape\n",
    "    mask = torch.zeros((bs, n, n), dtype=torch.bool)\n",
    "    nz = seq_mask.nonzero()\n",
    "    a, b = nz[:, 0], nz[:, 1]\n",
    "    mask[a, :, b] = True\n",
    "    if n_heads > 1:\n",
    "        mask = mask.repeat(1, n_heads, 1)\n",
    "        mask = mask.view(bs * n_heads, n, n)\n",
    "    return mask\n",
    "\n",
    "def generate_attn_mask(seq_mask, n_heads=1):\n",
    "    if len(seq_mask.shape) == 1:\n",
    "        return _generate_attn_mask_single(seq_mask, n_heads)\n",
    "    elif len(seq_mask.shape) == 2:\n",
    "        return _generate_attn_mask_batch(seq_mask, n_heads)\n",
    "    else:\n",
    "        assert False, f\"Input should be BATCH_SIZE * SEQ_LEN matrix, got {seq_mask.shape}\"\n",
    "            \n",
    "mask = generate_attn_mask(torch.Tensor([[False, False, True, False, True], [False, False, False, False, True]]))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d4c616-62ca-43a4-8011-fa48e10333d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "class PlaylistDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, files, playlist_per_file, transform=None):\n",
    "        self.files = files\n",
    "        self.current_file_index = -1\n",
    "        self.data = None\n",
    "        self.ppf = playlist_per_file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.ppf * len(self.files)\n",
    "    \n",
    "    def _load(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_index = index // self.ppf\n",
    "        offset = index % self.ppf\n",
    "        if self.current_file_index != file_index:\n",
    "            logging.debug(f\"Loading file {self.files[file_index]}\")\n",
    "            self._load(self.files[file_index])\n",
    "            self.current_file_index = file_index\n",
    "        tracks = self.data[\"playlists\"][offset]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            tracks = self.transform(tracks)\n",
    "        \n",
    "        return tracks\n",
    "\n",
    "    \n",
    "class Compose:\n",
    "    \n",
    "    def __init__(self, *tfs):\n",
    "        self.tfs = tfs\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for tf in self.tfs:\n",
    "            x = tf(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RemoveUnknownTracks:\n",
    "    \n",
    "    def __init__(self, known_tracks):\n",
    "        kt = known_tracks\n",
    "        if not isinstance(kt, set):\n",
    "            kt = set(kt)\n",
    "        self.kt = kt\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [xi for xi in x if xi in self.kt]\n",
    "    \n",
    "    \n",
    "class TrackURI2Idx:\n",
    "    \n",
    "    def __init__(self, uri2idx, offset=0):\n",
    "        self.offset = offset\n",
    "        self.uri2idx = uri2idx\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [self.uri2idx[xi] + self.offset for xi in x]\n",
    "    \n",
    "    \n",
    "class ToLongTensor:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return torch.LongTensor(x)\n",
    "    \n",
    "class PadOrTrim:\n",
    "    \n",
    "    def __init__(self, pad_token, target_length):\n",
    "        self.token = pad_token\n",
    "        self.t = target_length\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if len(x) == self.t:\n",
    "            return x\n",
    "        if len(x) < self.t:\n",
    "            return x + [self.token] * (self.t - len(x))\n",
    "        return x[:self.t]\n",
    "    \n",
    "    \n",
    "class MaskTracksTensor:\n",
    "    \n",
    "    def __init__(self, mask_token, padding_token, mask_proba):\n",
    "        self.token = mask_token\n",
    "        self.padding_token = padding_token\n",
    "        self.proba = mask_proba\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        mask = torch.rand(x.shape[0]) < self.proba\n",
    "        padding = x == self.padding_token\n",
    "        # avoid masking padded tracks\n",
    "        mask = mask & (~padding)\n",
    "        x_ = x.clone()\n",
    "        x_[mask] = self.token\n",
    "        return x_, x, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439e89b4-6aaf-4f77-8cae-7336214af09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../playlists_data/chunk_0.json',\n",
       " '../playlists_data/chunk_1.json',\n",
       " '../playlists_data/chunk_2.json',\n",
       " '../playlists_data/chunk_3.json',\n",
       " '../playlists_data/chunk_4.json',\n",
       " '../playlists_data/chunk_5.json',\n",
       " '../playlists_data/chunk_6.json',\n",
       " '../playlists_data/chunk_7.json',\n",
       " '../playlists_data/chunk_8.json',\n",
       " '../playlists_data/chunk_9.json',\n",
       " '../playlists_data/chunk_10.json',\n",
       " '../playlists_data/chunk_11.json',\n",
       " '../playlists_data/chunk_12.json',\n",
       " '../playlists_data/chunk_13.json',\n",
       " '../playlists_data/chunk_14.json',\n",
       " '../playlists_data/chunk_15.json',\n",
       " '../playlists_data/chunk_16.json',\n",
       " '../playlists_data/chunk_17.json',\n",
       " '../playlists_data/chunk_18.json',\n",
       " '../playlists_data/chunk_19.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f\"../playlists_data/chunk_{i}.json\" for i in range(20)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32ec579d-0321-4bab-a02a-8b0f5db81953",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "MASK_TOKEN = 1\n",
    "\n",
    "transforms = Compose(\n",
    "    RemoveUnknownTracks(wv.key_to_index.keys()),\n",
    "    TrackURI2Idx(wv.key_to_index, offset=2),\n",
    "    PadOrTrim(PAD_TOKEN, 5),\n",
    "    ToLongTensor(),\n",
    "    MaskTracksTensor(MASK_TOKEN, PAD_TOKEN, .1)\n",
    ")\n",
    "\n",
    "ds = PlaylistDataset(files, 50000, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f099e0f-aa9a-404b-856b-49fc884a775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a7bee8b-d5e1-497f-9856-fdee8cd0a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eea83a57-d0db-4b7a-8e52-9748dec9ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5680137-544f-40bd-850e-a04cc37cdccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "\n",
    "# TODO: add final linear layer\n",
    "\n",
    "GELU = nn.GELU()\n",
    "\n",
    "class TransRec(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, wv_model, n_head, layer_kwargs={}, enc_kwargs={}):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embd = Embedding(len(wv_model)+2, wv_model.vector_size) # +2 for <PAD> and <MASK> tokens\n",
    "        self.embd.weight.data[2:].copy_(torch.tensor(wv_model.vectors, dtype=torch.float))\n",
    "        self.embd.requires_grad_ = False\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(wv_model.vector_size, batch_first=True, \\\n",
    "                                                nhead=n_head, **layer_kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.encoder = TransformerEncoder(encoder_layer, **enc_kwargs)\n",
    "        self.linear = nn.Linear(wv_model.vector_size, wv_model.vector_size, bias=True)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embd(x)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return GELU(self.linear(x))\n",
    "    \n",
    "    def _token_probs(self, x, mask):\n",
    "        bs, seq_len, embd_dim = x.shape\n",
    "        num_tokens = self.embd.weight.shape[0]\n",
    "        x = x[mask, :]\n",
    "        logits = torch.matmul(self.embd.weight, x.view(-1, embd_dim).T).view(num_tokens, -1)\n",
    "        return logits.softmax(dim=0)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, mask = batch\n",
    "        padding_mask = x == PAD_TOKEN\n",
    "        attn_mask = generate_attn_mask(mask, n_heads=self.n_head)\n",
    "        \n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        y_ = self._token_probs(self.forward(x, mask=attn_mask), mask)\n",
    "        loss = crit(y_.T, y[mask])\n",
    "        print(loss.item())\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e27f9-d89e-4a5a-aa1f-021b0ac6c5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0ea6efc-a4f2-4e98-a230-b207d5776d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = TransRec(wv, n_head=10, enc_kwargs={\"num_layers\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2cdb434-6377-4a3e-b769-a4c221e55bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b4e5e11-9b73-4fac-9e34-c9f3f6c33bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | embd    | Embedding          | 28.1 M\n",
      "1 | encoder | TransformerEncoder | 1.4 M \n",
      "2 | linear  | Linear             | 10.1 K\n",
      "-----------------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.959   Total estimated model params size (MB)\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | embd    | Embedding          | 28.1 M\n",
      "1 | encoder | TransformerEncoder | 1.4 M \n",
      "2 | linear  | Linear             | 10.1 K\n",
      "-----------------------------------------------\n",
      "29.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.5 M    Total params\n",
      "117.959   Total estimated model params size (MB)\n",
      "2023-04-06 18:10:03.527757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-06 18:10:03.527894: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-06 18:10:03.527908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/mbh/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                        | 0/31250 [00:00<?, ?it/s]12.546979904174805\n",
      "Epoch 0:   0%|                     | 1/31250 [00:00<8:06:05,  1.07it/s, v_num=8]12.546850204467773\n",
      "Epoch 0:   0%|                     | 2/31250 [00:01<8:07:56,  1.07it/s, v_num=8]12.546832084655762\n",
      "Epoch 0:   0%|                     | 3/31250 [00:02<8:32:59,  1.02it/s, v_num=8]12.546906471252441\n",
      "Epoch 0:   0%|                     | 4/31250 [00:04<9:08:35,  1.05s/it, v_num=8]12.54690933227539\n",
      "Epoch 0:   0%|                    | 5/31250 [00:05<10:04:56,  1.16s/it, v_num=8]12.54688835144043\n",
      "Epoch 0:   0%|                    | 6/31250 [00:07<11:00:22,  1.27s/it, v_num=8]12.54714584350586\n",
      "Epoch 0:   0%|                    | 7/31250 [00:09<11:35:40,  1.34s/it, v_num=8]12.547018051147461\n",
      "Epoch 0:   0%|                    | 8/31250 [00:11<12:05:04,  1.39s/it, v_num=8]12.547054290771484\n",
      "Epoch 0:   0%|                    | 9/31250 [00:14<13:34:53,  1.57s/it, v_num=8]12.54702377319336\n",
      "Epoch 0:   0%|                   | 10/31250 [00:16<14:10:14,  1.63s/it, v_num=8]12.547025680541992\n",
      "Epoch 0:   0%|                   | 11/31250 [00:19<15:05:53,  1.74s/it, v_num=8]12.547026634216309\n",
      "Epoch 0:   0%|                   | 12/31250 [00:21<15:23:52,  1.77s/it, v_num=8]12.547022819519043\n",
      "Epoch 0:   0%|                   | 13/31250 [00:23<15:33:21,  1.79s/it, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbh/.venvs/base/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(tr, train_dataloaders=dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c611c7bd-3419-4b83-8187-97695df9fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10405103 / 281219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a938f1af-14af-4787-bfbd-ddbd98494814",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, mask = batch\n",
    "padding_mask = x == PAD_TOKEN\n",
    "attn_mask = generate_attn_mask(mask, n_heads=tr.n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45aebf35-7f51-4faf-9dea-26bbc1d87c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tr._token_probs(tr(x, mask=attn_mask), mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d1b80c-761a-4eb4-b9d0-7cd9b855f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ce1506f-6a5b-42eb-a030-60c1d3a95df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = crit(out.T, y[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d9f8db4-7701-4056-8de8-0a7d3fc33eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.5469, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7667a-9ff4-421c-bb70-da60dc757d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
