{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3429df3-f0c2-4a43-9eb1-e8f8813e44d9",
   "metadata": {},
   "source": [
    "# Mathematical Statistics Homework Assignment\n",
    "#### 2023.05\n",
    "#### RichÃ¡rd Kiss - KAYXFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda3007-0192-474b-b23b-e0190b7ffe21",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this assignment I use a dataset that was generated as part of my PhD research. The dataset contains music playlists and song recommendations for these playlists generated with two different algorithms. In this assignment I am performing statistical analysis to compare these two algorithms in terms of recommendation performance.\n",
    "\n",
    "## The two recommendation algorithms\n",
    "### Algorithm 1.\n",
    "The first algorithm recommends songs based solely on their popularity. It consistently returns the top $K$ most popular songs, irrespective of the particular songs present in the playlist for which we are making new song recommendations. Despite its simplicity, this approach is very effective and often used as a benchmark in the evaluation of recommendation systems.\n",
    "\n",
    "### Algorithm 2.\n",
    "The second algorithm employs Skip-Gram with negative sampling to learn insightful, low-dimensional song representations (denoted by $w_i \\in \\mathbb{R}^l$ for all songs $i$, where $l$ is the dimensionality of the representation). The core concept revolves around discerning whether song pairs originate from the original data distribution, $P_{orig}$, or from a synthetic noise distribution, $P_{noise}$.\n",
    "\n",
    "The song pairs in the original distribution are generated by sampling songs from a playlist within a specific context window (of size $c$), and corresponding noise samples are created by randomly pairing songs that do not appear together within a context window (of the same size $c$).\n",
    "\n",
    "To reduce computational demands, we ease the restriction of never appearing together. Since the co-appearance graph of songs is usually sparse, choosing two random songs seldom results in a pair that co-appears in the dataset. Although there's still a slim chance, this compromise generally speeds up the sampling process without drastically affecting the results. (You can find more details about this method  [here](https://www.baeldung.com/cs/nlps-word2vec-negative-sampling)).\n",
    "\n",
    "Once we've learned the song representations $w$, we can use them to generate playlist recommendations. The process begins by selecting the $K$ most similar songs for each song in the playlist. Then, we count the number of times a particular song appears within the $K$ proximity of each song in the playlist and rank them in descending order. Finally, we return the top $K$ songs from this sorted list.\n",
    "\n",
    "\n",
    "## The data\n",
    "spotify dataset, #>30, some basic statistics (num songs, albums, artists etc.)\n",
    "\n",
    "\n",
    "# 2. Statistical analysis\n",
    "## Hypotheses\n",
    "__First Hypothesis__\n",
    "\n",
    "I want to check whether the frequencies of songs (number of appearances in playlists) comes from an exponential distribution. To test this I first fit an exponential distribution on the frequencies then I perform a one sample Kolmogorov-Smirnov test with the fitted exponential.\n",
    "\n",
    "__Second Hypothesis__\n",
    "\n",
    "Recommendations generated with Algorithm 2. have higher recall ($\\frac{TP}{TP+FN}$) on average than those generated with Algorithm 1. To test this I am performing a two sample T-test on the recall values.\n",
    "\n",
    "__Regression analysis__\n",
    "\n",
    "...\n",
    "## The data I use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa69006-40ea-4398-80bb-cafb1ac6e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ad1b4c-1452-499e-97d5-928ffd3cf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257ab9c9-d6c3-4400-b5f0-e4c4a1281588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(*path):\n",
    "    with open(os.path.join(*path)) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "idx2song = load(DATA_FOLDER, \"metadata\", \"idx2song.json\")\n",
    "freqs = load(DATA_FOLDER, \"metadata\", \"frequencies.json\")\n",
    "# it is necessary to remap song id-s to song names in the freqs json\n",
    "freqs = {idx2song[k]: v for k,v in freqs.items()}\n",
    "algo1 = load(DATA_FOLDER, \"popularity.json\")\n",
    "algo2 = load(DATA_FOLDER, \"euclidean.json\")\n",
    "qrels = load(DATA_FOLDER, \"qrels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d3d194-b80c-400a-89b0-5ff46aff6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp, fn):\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def get_recalls(qrels, recs):\n",
    "    for (q_id, q_data), (r_id, r_data) in zip(qrels.items(), recs.items()):\n",
    "        assert q_id == r_id, f\"ID mismatch {q_id} {r_id}\"\n",
    "        actual = set(q_data.keys())\n",
    "        predicted = set(r_data.keys())\n",
    "        tp = len(actual & predicted)\n",
    "        fn = len(actual.difference(predicted))\n",
    "        yield recall(tp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dabd5f0-d9bf-4444-ab82-62e4d1467343",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls1 = list(get_recalls(qrels, algo1))\n",
    "recalls2 = list(get_recalls(qrels, algo2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a351f0-3665-4d8f-913f-39c7f8c6fa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=-246.58068655214413, pvalue=0.0, df=84791)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(recalls1, recalls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f230ea2-dba1-41f2-b75d-2901047eeb54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
